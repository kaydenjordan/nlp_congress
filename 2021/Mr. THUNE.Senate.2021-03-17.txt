 Madam President, I ask unanimous consent that I be able to  complete my remarks before the vote.    I yield the floor    Mr. President, social media platforms have become a pretty  significant part of Americans' lives. We use them to stay up to date on  news from friends and family--something that has become especially  essential during the pandemic--to communicate with relatives and  friends, for entertainment, and as a shopping resource. Social media  sites provide ways to network, to connect with like-minded individuals  from fellow theater lovers to fellow basketball fans, to advocate for  causes that we believe in, to conduct business, even to date, and more  and more we rely on social media sites as a primary source of news and  information, from Presidential election news to updates on COVID  vaccinations.   Social media offers a lot of benefits and opportunities, but the  increasing dominance of social media, particularly in the news and  information space, has also raised concerns. Consumers have become  increasingly troubled about the way their information is used by social  media platforms and how these sites decide what news and information we  see. And there are increasing numbers of anecdotes to suggest that some  social media platforms are moderating content in a biased or political  way.   Currently, content moderation on social media platforms is governed  by section 230 of the Communications Decency Act, which was enacted  into law 25 years ago. Section 230 provides internet sites that host  user-generated content--sites like YouTube or Twitter or Facebook--with  immunity for the content that users post on their sites. So, for  example, if somebody posts a video on YouTube that contains illegal  content, YouTube isn't held legally responsible for that content.   Section 230 has been critical to the development of the internet as  we know it today. Without section 230 protections, many of the sites we  rely on for social connection or news or entertainment would never have  come into being.  [[Page S1616]]    But as the internet and social media have grown and developed, it has  also become clear that some changes need to be made. In particular, it  has become increasingly clear that sites need to provide greater  transparency when it comes to their content moderation practices and  decisions. Social media sites are no longer just providing a platform  for user-generated content as they did in their infancy. They are now  making a lot of decisions about that content and carefully shaping our  social media experience--what ads we see, what posts we see, what news  stories we see.   Currently, Federal law does not require that social media sites be at  all accountable to consumers for those content moderation decisions.  That is why, today, I am introducing the Platform Accountability and  Consumer Transparency Act, or the PACT Act, along with my colleague  Senator Schatz. Our bill would preserve the benefits of section 230,  like the internet growth and widespread dissemination of free speech it  has enabled, while increasing accountability and consumer transparency  around content moderation.   Now, content moderation is certainly not all bad. For example, most  of us are happy to have YouTube or Instagram suggest additional content  that matches the music that we like to listen to or the hobbies that we  are interested in. The problem is that content moderation has been and  largely continues to be a black box, with consumers having little or no  idea how the information they see has been shaped by the sites that  they are visiting.   The PACT Act would address this problem by increasing transparency  around the content moderation process. Sites would be required to  provide an easily digestible disclosure of their content moderation  practices for users, and, importantly, they would be required to  explain their decisions to remove material to consumers.   Until relatively recently, sites like Facebook and Twitter would  remove a user's post without explanation and without an appeals  process. And even as platforms start to shape up their act with regard  to transparency and due process, it is still hard for users to get good  information about how content is moderated.   Under the PACT Act, if a site chooses to remove your post, it has to  tell you why it decided to remove your post and explain how your post  violated the site's terms of use. The PACT Act would also require sites  to have an appeals process. So if Facebook, for example, removes one of  your posts, it would not only have to tell you why, but it would have  to provide a way for you to appeal that decision.   We have seen increased concern lately about news articles being  removed from social media sites. Under the PACT Act, a newspaper whose  article was posted on Facebook or Twitter and then removed by one of  those platforms could challenge Facebook or Twitter, which would have  to provide a reason for removing the article and allow the newspaper to  appeal the decision.   The PACT Act would also help us develop the data necessary to  demonstrate whether social media platforms are removing content in a  biased or political fashion. As I said earlier, there has been  increasing concern about biased content moderation on social media  sites. The PACT Act requires detailed transparency reports every 6  months from large social media platforms, like Twitter and Facebook,  which will provide the data it needed to determine whether and where  biased moderation exists.   The PACT Act would also bolster efforts by State governments to hold  social media platforms accountable. The bill would allow State  attorneys general to bring civil lawsuits against social media  platforms when these platforms have violated Federal civil laws.   The PACT Act would also require companies to remove material that has  been adjudicated as illegal by a court. Internet platforms would be  required to remove illegal content within 4 days. Failure to remove  illegal material would result in the platform's losing its 230  protections for that content or activity, a provision that matches a  recommendation made by the Trump Department of Justice for section 230  reform.   I am grateful to Senator Schatz for partnering with me on this  legislation. Our bill is a serious, bipartisan approach to the issue of  section 230 reform, and it would go a long way toward making social  media platforms more accountable to consumers and increasing  transparency around the content moderation process.   I invite our colleagues on both sides of the aisle to join us in  advancing this legislation.                                  ______                                         By Ms. COLLINS:   S. 804. A bill to amend the Internal Revenue Code of 1986 to increase  the limitation on the amount individuals filing jointly can deduct for  certain State and local taxes; to the Committee on Finance.    Mr. President, social media platforms have become a pretty  significant part of Americans' lives. We use them to stay up to date on  news from friends and family--something that has become especially  essential during the pandemic--to communicate with relatives and  friends, for entertainment, and as a shopping resource. Social media  sites provide ways to network, to connect with like-minded individuals  from fellow theater lovers to fellow basketball fans, to advocate for  causes that we believe in, to conduct business, even to date, and more  and more we rely on social media sites as a primary source of news and  information, from Presidential election news to updates on COVID  vaccinations.   Social media offers a lot of benefits and opportunities, but the  increasing dominance of social media, particularly in the news and  information space, has also raised concerns. Consumers have become  increasingly troubled about the way their information is used by social  media platforms and how these sites decide what news and information we  see. And there are increasing numbers of anecdotes to suggest that some  social media platforms are moderating content in a biased or political  way.   Currently, content moderation on social media platforms is governed  by section 230 of the Communications Decency Act, which was enacted  into law 25 years ago. Section 230 provides internet sites that host  user-generated content--sites like YouTube or Twitter or Facebook--with  immunity for the content that users post on their sites. So, for  example, if somebody posts a video on YouTube that contains illegal  content, YouTube isn't held legally responsible for that content.   Section 230 has been critical to the development of the internet as  we know it today. Without section 230 protections, many of the sites we  rely on for social connection or news or entertainment would never have  come into being.  [[Page S1616]]    But as the internet and social media have grown and developed, it has  also become clear that some changes need to be made. In particular, it  has become increasingly clear that sites need to provide greater  transparency when it comes to their content moderation practices and  decisions. Social media sites are no longer just providing a platform  for user-generated content as they did in their infancy. They are now  making a lot of decisions about that content and carefully shaping our  social media experience--what ads we see, what posts we see, what news  stories we see.   Currently, Federal law does not require that social media sites be at  all accountable to consumers for those content moderation decisions.  That is why, today, I am introducing the Platform Accountability and  Consumer Transparency Act, or the PACT Act, along with my colleague  Senator Schatz. Our bill would preserve the benefits of section 230,  like the internet growth and widespread dissemination of free speech it  has enabled, while increasing accountability and consumer transparency  around content moderation.   Now, content moderation is certainly not all bad. For example, most  of us are happy to have YouTube or Instagram suggest additional content  that matches the music that we like to listen to or the hobbies that we  are interested in. The problem is that content moderation has been and  largely continues to be a black box, with consumers having little or no  idea how the information they see has been shaped by the sites that  they are visiting.   The PACT Act would address this problem by increasing transparency  around the content moderation process. Sites would be required to  provide an easily digestible disclosure of their content moderation  practices for users, and, importantly, they would be required to  explain their decisions to remove material to consumers.   Until relatively recently, sites like Facebook and Twitter would  remove a user's post without explanation and without an appeals  process. And even as platforms start to shape up their act with regard  to transparency and due process, it is still hard for users to get good  information about how content is moderated.   Under the PACT Act, if a site chooses to remove your post, it has to  tell you why it decided to remove your post and explain how your post  violated the site's terms of use. The PACT Act would also require sites  to have an appeals process. So if Facebook, for example, removes one of  your posts, it would not only have to tell you why, but it would have  to provide a way for you to appeal that decision.   We have seen increased concern lately about news articles being  removed from social media sites. Under the PACT Act, a newspaper whose  article was posted on Facebook or Twitter and then removed by one of  those platforms could challenge Facebook or Twitter, which would have  to provide a reason for removing the article and allow the newspaper to  appeal the decision.   The PACT Act would also help us develop the data necessary to  demonstrate whether social media platforms are removing content in a  biased or political fashion. As I said earlier, there has been  increasing concern about biased content moderation on social media  sites. The PACT Act requires detailed transparency reports every 6  months from large social media platforms, like Twitter and Facebook,  which will provide the data it needed to determine whether and where  biased moderation exists.   The PACT Act would also bolster efforts by State governments to hold  social media platforms accountable. The bill would allow State  attorneys general to bring civil lawsuits against social media  platforms when these platforms have violated Federal civil laws.   The PACT Act would also require companies to remove material that has  been adjudicated as illegal by a court. Internet platforms would be  required to remove illegal content within 4 days. Failure to remove  illegal material would result in the platform's losing its 230  protections for that content or activity, a provision that matches a  recommendation made by the Trump Department of Justice for section 230  reform.   I am grateful to Senator Schatz for partnering with me on this  legislation. Our bill is a serious, bipartisan approach to the issue of  section 230 reform, and it would go a long way toward making social  media platforms more accountable to consumers and increasing  transparency around the content moderation process.   I invite our colleagues on both sides of the aisle to join us in  advancing this legislation.                                  ______   